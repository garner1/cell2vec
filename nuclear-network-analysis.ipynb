{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load .mat files with pymatreader'''\n",
    "from pymatreader import read_mat\n",
    "filepath = '/home/garner1/Downloads/im_structured_nuclei_sparse.mat'\n",
    "data = read_mat(filepath) # it takes for ever to load but it works\n",
    "# len(data['structured_im_array'])\n",
    "data['structured_im_array'][0]\n",
    "for ind in range(len(data['structured_im_array'])):\n",
    "    if len(data['structured_im_array'][ind]) > 0:\n",
    "        print(ind,len(data['structured_im_array'][ind]['SparseMatrix']))\n",
    "\n",
    "from scipy import sparse\n",
    "M = sparse.csc_matrix( (data['structured_im_array'][0]['SparseMatrix']['data'], \n",
    "                        data['structured_im_array'][0]['SparseMatrix']['ir'],\n",
    "                        data['structured_im_array'][0]['SparseMatrix']['jc']) )\n",
    "# print(M)\n",
    "x,y = data['structured_im_array'][0]['Centroid']\n",
    "# print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load .mat files with loadmat'''\n",
    "from scipy.io import loadmat\n",
    "filepath = '/home/garner1/Downloads/single_fov_2_1.mat'\n",
    "data = loadmat(filepath)\n",
    "\n",
    "import numpy as np\n",
    "print(data['fov_2_1'][0])\n",
    "data['fov_2_1'][0][-1].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy import ndarray\n",
    "\n",
    "import skimage as sk\n",
    "from skimage import transform\n",
    "from skimage import util\n",
    "from skimage import io\n",
    "import imageio\n",
    "import cv2\n",
    "\n",
    "from numpy import linalg as LA\n",
    "from scipy.sparse import csc_matrix,coo_matrix\n",
    "from scipy.sparse.linalg import svds, eigs,expm\n",
    "from sklearn.decomposition import PCA,TruncatedSVD\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import random\n",
    "\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN, KMeans, AffinityPropagation, MeanShift\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "import hdbscan\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy import ndimage\n",
    "import imageio\n",
    "\n",
    "import csv\n",
    "import igraph\n",
    "from igraph import *\n",
    "\n",
    "def embedding(data,dim):\n",
    "    projection = mapper.fit_transform(data, projection=umap.UMAP(n_components=dim, n_neighbors=200, \n",
    "                                             a=None, angular_rp_forest=False, b=None, init='spectral',\n",
    "                                           learning_rate=1.0, local_connectivity=1.0, metric='euclidean',\n",
    "                                           metric_kwds=None, min_dist=0.1, n_epochs=500,\n",
    "                                           negative_sample_rate=10, random_state=47,\n",
    "                                           repulsion_strength=1.0, set_op_mix_ratio=0.5, spread=0.25,\n",
    "                                           target_metric='categorical', target_metric_kwds=None,\n",
    "                                           target_n_neighbors=-1, target_weight=0.5, transform_queue_size=10.0,\n",
    "                                           transform_seed=42, verbose=False))\n",
    "    return projection\n",
    "\n",
    "def bbox(img):\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    if np.where(rows)[0].size*np.where(cols)[0].size > 0:\n",
    "        rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "        cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    else:\n",
    "        rmin, rmax = 0, 1\n",
    "        cmin, cmax = 0, 1\n",
    "    return rmin, rmax, cmin, cmax    \n",
    "\n",
    "def pad_with(vector, pad_width, iaxis, kwargs):\n",
    "    pad_value = kwargs.get('padder', 0)\n",
    "    vector[:pad_width[0]] = pad_value\n",
    "    vector[-pad_width[1]:] = pad_value\n",
    "\n",
    "def rotate_image(mat, angle):\n",
    "    \"\"\"\n",
    "    Rotates an image (angle in degrees) and expands image to avoid cropping\n",
    "    \"\"\"\n",
    "\n",
    "    height, width = mat.shape[:2] # image shape has 3 dimensions\n",
    "    image_center = (width/2, height/2) # getRotationMatrix2D needs coordinates in reverse order (width, height) compared to shape\n",
    "\n",
    "    rotation_mat = cv2.getRotationMatrix2D(image_center, angle, 1.)\n",
    "\n",
    "    # rotation calculates the cos and sin, taking absolutes of those.\n",
    "    abs_cos = abs(rotation_mat[0,0]) \n",
    "    abs_sin = abs(rotation_mat[0,1])\n",
    "\n",
    "    # find the new width and height bounds\n",
    "    bound_w = int(height * abs_sin + width * abs_cos)\n",
    "    bound_h = int(height * abs_cos + width * abs_sin)\n",
    "\n",
    "    # subtract old image center (bringing image back to origo) and adding the new image center coordinates\n",
    "    rotation_mat[0, 2] += bound_w/2 - image_center[0]\n",
    "    rotation_mat[1, 2] += bound_h/2 - image_center[1]\n",
    "\n",
    "    # rotate image with the new bounds and translated rotation matrix\n",
    "    rotated_mat = cv2.warpAffine(mat, rotation_mat, (bound_w, bound_h))\n",
    "    return rotated_mat\n",
    "\n",
    "import math\n",
    "\n",
    "def round_up_to_even(f):\n",
    "    return int(math.ceil(f / 2.) * 2)\n",
    "\n",
    "def edges_rescaling(edges): # edges are mat.data where mat is a sparse scipy matrix\n",
    "    edges = np.log10(edges) # log rescale weights because they vary over many decades\n",
    "    edges -= min(edges) # make them positive \n",
    "    edges /= max(edges)*(1.0/10) # rescale from 0 to 10\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = 164\n",
    "directory = '/home/garner1/Work/dataset/dataset_patient52/segmented_nuclei_'+str(img_id)\n",
    "\n",
    "Mwidths = 60\n",
    "Mheights = 60\n",
    "images = []\n",
    "cell_id = []\n",
    "\n",
    "count = 1\n",
    "while count <= len(os.listdir(directory)):\n",
    "    cell = 'iimage_sub'+str(img_id)+'_cell_no_'+str(count)+'.jpg'\n",
    "    filename = os.path.join(directory, cell)\n",
    "    img = imageio.imread(filename)\n",
    "    rmin, rmax, cmin, cmax = bbox(img)\n",
    "    delta_w, delta_h = Mwidths-(rmax-rmin), Mheights-(cmax-cmin)\n",
    "    if delta_w >= 0 and delta_h >= 0:\n",
    "        top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "        left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "        newimg = np.pad(img[rmin:rmax,cmin:cmax],((left,right),(top,bottom)),'constant', constant_values=(0))\n",
    "        images.append(newimg)\n",
    "        cell_id.append(count)\n",
    "    else:\n",
    "        print(filename+' has a problem!')\n",
    "    count += 1\n",
    "\n",
    "\n",
    "with open('/home/garner1/Work/dataset/dataset_patient52/centroids_'+str(img_id)+'.txt', 'r') as f:\n",
    "    properties = list(csv.reader(f, delimiter=','))\n",
    "XY = np.array(properties)[1:,0:2].astype(np.float) # only select x,y coordinates\n",
    "print('The shape of the geometric data is '+str(XY.shape))\n",
    "\n",
    "X = np.zeros((len(images),Mwidths*Mheights))\n",
    "for ind in range(len(images)):\n",
    "    X[ind,:] = images[ind].flatten() # from 2D arrays to 1D arrays\n",
    "print('The shape of the morphology data is '+str(X.shape))\n",
    "\n",
    "'''\n",
    "Returns\n",
    "-------\n",
    "fuzzy_simplicial_set: coo_matrix\n",
    "A fuzzy simplicial set represented as a sparse matrix. The \n",
    "(i,j) entry of the matrix represents the membership strength of the\n",
    "1-simplex between the ith and jth sample points.\n",
    "'''\n",
    "mat_X = umap.umap_.fuzzy_simplicial_set(\n",
    "    X,\n",
    "    n_neighbors=50, # this will affect sparsity of the final Hadamard graph\n",
    "    random_state=np.random.RandomState(seed=42),\n",
    "    metric='l1',  # use L1 norm to compare morphologies, seems to work better\n",
    "    metric_kwds={},\n",
    "    knn_indices=None,\n",
    "    knn_dists=None,\n",
    "    angular=False,\n",
    "    set_op_mix_ratio=1.0,\n",
    "    local_connectivity=2.0,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "mat_XY = umap.umap_.fuzzy_simplicial_set(\n",
    "    XY,\n",
    "    n_neighbors=50, # this will affect sparsity of the final Hadamard graph\n",
    "    random_state=np.random.RandomState(seed=42),\n",
    "    metric='l2',\n",
    "    metric_kwds={},\n",
    "    knn_indices=None,\n",
    "    knn_dists=None,\n",
    "    angular=False,\n",
    "    set_op_mix_ratio=1.0,\n",
    "    local_connectivity=2.0,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "hada = mat_X.multiply(mat_XY)\n",
    "# hada = hada.__pow__(10) # weighted number of length-n paths connecting i to j\n",
    "\n",
    "hada.data = edges_rescaling(hada.data) # rescale to a log-scale from 0 to 10\n",
    "# mat_X.data = edges_rescaling(mat_X.data)\n",
    "# mat_XY.data = edges_rescaling(mat_XY.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sparsity of the graph is '+str(100*hada.nnz*1.0/hada.shape[0]**2)+'%')\n",
    "\n",
    "plt.hist(hada.data)\n",
    "plt.xticks(size = 50)\n",
    "plt.yticks(size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The edges weight are log-gaussian distributed\n",
    "A log-normal process is the statistical realization \n",
    "of the multiplicative product of many independent random variables, each of which is positive\n",
    "'''\n",
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "n, bins, patches = ax.hist(hada.data, bins=1000, density=True, histtype='step',cumulative=True)\n",
    "ax.hist(hada.data, bins=1000, density=True, histtype='step', cumulative=-1,label='Reversed emp.')\n",
    "plt.xticks(size = 50)\n",
    "plt.yticks(size = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', rc={'figure.figsize':(50,50)})\n",
    "img = cv2.imread('/home/garner1/Work/dataset/dataset_patient52/iMS266_20190426_001.sub'+str(img_id)+'.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "G = nx.from_scipy_sparse_matrix(hada) # if sparse matrix\n",
    "\n",
    "eset = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] >= 0]\n",
    "weights = [d['weight'] for (u, v, d) in G.edges(data=True)]\n",
    "pos = XY\n",
    "\n",
    "# nx.draw_networkx_nodes(G, pos,alpha=0.0)\n",
    "# nx.draw_networkx_edges(G, pos, edgelist=eset,alpha=0.25, width=weights,edge_color='r',style='solid')\n",
    "# plt.axis('off')\n",
    "# plt.imshow(img,cmap='gray')\n",
    "# plt.savefig(str(img_id)+'graph.png',bbox_inches='tight')\n",
    "# plt.close()\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos,alpha=1.0)\n",
    "nx.draw_networkx_edges(G, pos, edgelist=eset,alpha=1.0, width=weights,edge_color='r',style='solid')\n",
    "plt.axis('off')\n",
    "plt.savefig(str(img_id)+'graph_only.png',bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# plt.imshow(img,cmap='gray')\n",
    "# plt.savefig(str(img_id)+'dapi.png',bbox_inches='tight')\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources, targets = hada.nonzero()\n",
    "edgelist = list(zip(sources.tolist(), targets.tolist()))\n",
    "g = Graph(edgelist,edge_attrs={'weight': hada.data.tolist()})\n",
    "g = g.simplify(combine_edges=mean)\n",
    "comm = g.community_fastgreedy(weights=g.es[\"weight\"]) #gives overlapping communities\n",
    "clust = comm.as_clustering()\n",
    "\n",
    "big_clusters = [ind for ind in range(len(clust)) if len(clust[ind])>=1] #filter clusters by size\n",
    "number_of_colors = len(big_clusters)\n",
    "color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "             for i in range(number_of_colors)]\n",
    "\n",
    "sns.set(style='white', rc={'figure.figsize':(50,50)})\n",
    "count = 0\n",
    "for i in big_clusters:\n",
    "    List = [color[count],]*len(clust[i])\n",
    "    nx.draw_networkx_nodes(G, pos,nodelist=clust[i],alpha=0.9,node_color=List,node_size=1000)\n",
    "    count += 1\n",
    "# nx.draw_networkx_edges(G, pos, edgelist=eset,alpha=0.75, width=weights,edge_color='r',style='solid')\n",
    "plt.axis('off')\n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.savefig(str(img_id)+'communities.png',bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# HDBSCAN clusters in 3D\n",
    "# low min sample size seems to refuce unclustered data;\n",
    "# larger min cluster size decrease cluster numbers\n",
    "# PCA reduction might not be a good idea because shape space is non-linear and the linear reduction could distort distances and later clustering\n",
    "# '''\n",
    "# # %time labels = hdbscan.HDBSCAN(min_samples=50,min_cluster_size=100).fit_predict(embedding)\n",
    "\n",
    "# from matplotlib.pyplot import figure\n",
    "# figure(num=None, figsize=(14, 12), dpi=80, facecolor='w', edgecolor='w',frameon=False)\n",
    "# # plt.scatter(embedding[:, 0], embedding[:, 1], s= 1, c=labels, cmap='Spectral')\n",
    "# plt.scatter(embedding[:, 0], embedding[:, 1], s= 1)\n",
    "# ##########\n",
    "# # clustered = (labels >= 0)\n",
    "# # print('The percentage of clustered data points is '+str(np.sum(clustered) *1.0/ X.shape[0]*100)+'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "\n",
    "# Precompute probabilities and generate walks - **ON WINDOWS ONLY WORKS WITH workers=1**\n",
    "node2vec = Node2Vec(G, dimensions=2, walk_length=20, num_walks=200, workers=24)  # Use temp_folder for big graphs\n",
    "\n",
    "# Embed nodes\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `diemnsions` and `workers` are automatically passed (from the Node2Vec constructor)\n",
    "\n",
    "%time embedding = np.asarray([model.wv[key] for idx, key in enumerate(model.wv.vocab)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
